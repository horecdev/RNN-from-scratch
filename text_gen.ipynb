{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import time\n",
    "\n",
    "from core import RNN, Embedding, SoftmaxCrossEntropy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_enc(y, num_classes):\n",
    "    # Y is (B, seq_len)\n",
    "    # We want (B, seq_len, vocab_size)\n",
    "    B, T = y.shape\n",
    "    \n",
    "    y_flat = y.ravel() # Flattens to 1D without copying memory\n",
    "    one_hot = cp.zeros((B * T, num_classes))\n",
    "    \n",
    "    one_hot[cp.arange(B * T), y_flat] = 1 # Both must be 1D arrays, go over pairs (a, b)\n",
    "    \n",
    "    return one_hot.reshape(B, T, num_classes)\n",
    "    \n",
    "\n",
    "class SherlockDataset:\n",
    "    def __init__(self, path, batch_size, seq_len):\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            self.text = file.read()\n",
    "            \n",
    "        allowed_chars = string.ascii_letters + string.digits + string.punctuation + \" \\n\"\n",
    "        \n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch:i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i:ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        \n",
    "        self.data = cp.array([self.stoi[ch] for ch in self.text if ch in allowed_chars]) # ndims = 1\n",
    "        \n",
    "        chars_elem = self.data.size // batch_size # how many chars in one element in batch\n",
    "        self.data = self.data[:chars_elem * batch_size] # cutoff\n",
    "        self.data = self.data.reshape(batch_size, -1)\n",
    "        \n",
    "        self.num_batches = (self.data.shape[1] - 1) // seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def get_batch(self, i):\n",
    "        start = i * self.seq_len\n",
    "        end = start + self.seq_len\n",
    "        \n",
    "        X = self.data[:, start:end]\n",
    "        Y = self.data[:, start + 1: end + 1]\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    # We assume logits are (B, seq_len, out_dim)\n",
    "    max_logits = cp.max(logits, axis=2, keepdims=True) # (B, seq_len, 1)\n",
    "    shifted_logits = logits - max_logits # (B, seq_len, out_dim)\n",
    "    \n",
    "    exp_logits = cp.exp(shifted_logits) # (B, seq_len, out_dim)\n",
    "    exp_sum = cp.sum(exp_logits, axis=2, keepdims=True) # (B, seq_len, 1)\n",
    "    \n",
    "    probs = exp_logits / exp_sum # (B, seq_len, out_dim)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def sample(models: list[RNN], embedding: Embedding, dataset: SherlockDataset, start_char: str, length: int, temperature:float = 0.8, time_breaks: float = 0.0) -> str:\n",
    "    current_ix = dataset.stoi[start_char]\n",
    "    h = [None for _ in models]\n",
    "    output_str = start_char\n",
    "    print(start_char, end=\"\")\n",
    "    \n",
    "    for _ in range(length):\n",
    "        x = embedding.forward(cp.array([[current_ix]])) \n",
    "        \n",
    "        for i in range(len(models)):\n",
    "            x, h[i] = models[i].forward(x, h[i])\n",
    "            \n",
    "        logits = x / temperature\n",
    "        probs_gpu = softmax(logits).reshape(-1)\n",
    "        \n",
    "        probs_cpu = cp.asnumpy(probs_gpu).astype('float64') # We convert to np to have working sampling (cupy is weird)\n",
    "        next_ix = np.random.choice(dataset.vocab_size, p=probs_cpu)\n",
    "        \n",
    "        next_character = dataset.itos[int(next_ix)]\n",
    "        output_str += next_character\n",
    "        current_ix = next_ix\n",
    "        \n",
    "        print(next_character, end=\"\", flush=True)\n",
    "        \n",
    "        if time_breaks != 0.0:\n",
    "            time.sleep(time_breaks)\n",
    "        \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e85a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'sherlock.txt'\n",
    "batch_size = 128\n",
    "seq_len = 128\n",
    "\n",
    "embed_dim = 64\n",
    "hidden_dim = 512\n",
    "\n",
    "epochs = 80\n",
    "learning_rate = 5e-4\n",
    "\n",
    "dataset = SherlockDataset(data_path, batch_size, seq_len)\n",
    "\n",
    "embedding = Embedding(dataset.vocab_size, embed_dim)\n",
    "models = [RNN(embed_dim, hidden_dim, hidden_dim), RNN(hidden_dim, hidden_dim, dataset.vocab_size)]\n",
    "loss_fn = SoftmaxCrossEntropy()\n",
    "\n",
    "h_states = [None for _ in models]\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_index in range(dataset.num_batches):\n",
    "        X, Y = dataset.get_batch(batch_index)\n",
    "        Y = one_hot_enc(Y, dataset.vocab_size)\n",
    "        x_emb = embedding.forward(X) # (B, seq_len, embed_dim)\n",
    "        activations = [x_emb] # Prepare for forwards pass \n",
    "        \n",
    "        for i in range(len(models)): # Essentially each RNN outputs (B, T, hidden_dim) so its some projection of input_dim\n",
    "            output, h_states[i] = models[i].forward(activations[-1], h_states[i])\n",
    "            activations.append(output)\n",
    "        \n",
    "        logits = activations[-1]\n",
    "        loss = loss_fn.forward(logits, Y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        dlogits = loss_fn.backward()\n",
    "        grads = [dlogits] # prepare for backwards pass\n",
    "        \n",
    "        for i in range(len(models)):\n",
    "            grad = models[len(models) - i - 1].backward(grads[-1])\n",
    "            grads.append(grad)\n",
    "            \n",
    "        embedding.backward(grads[-1])\n",
    "        \n",
    "        for m in models:\n",
    "            m.step(learning_rate)\n",
    "        embedding.step(learning_rate)\n",
    "            \n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Epoch: {epoch+1} | Loss: {loss}\")\n",
    "    sample(models, embedding, dataset, \"A\", 200, 0.8)\n",
    "    print(\"\\n\" + \"=\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb07644",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(models, embedding, dataset, \"H\", 200000, temperature=0.8, time_breaks=0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
