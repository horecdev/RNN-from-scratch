{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core import RNN, Embedding, SoftmaxCrossEntropy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyShakespeareDataset:\n",
    "    def __init__(self, path, batch_size, seq_len):\n",
    "        with open(path, 'r') as file:\n",
    "            self.text = file.read()\n",
    "        \n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = {ch:i for i, ch in enumerate(self.chars)}\n",
    "        self.itos = {i:ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        chars_elem = len(self.text) // batch_size # how many chars in one element in batch\n",
    "        self.data = self.text[:chars_elem * batch_size] # cutoff\n",
    "        \n",
    "        self.data = np.array([self.stoi[ch] for ch in self.text]) # ndims = 1\n",
    "        self.data = self.data.reshape(batch_size, -1)\n",
    "        \n",
    "        self.num_batches = (len(self.data) - 1) // seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def get_batch(self, i):\n",
    "        start = i * self.seq_len\n",
    "        end = start + self.seq_len\n",
    "        \n",
    "        X = self.data[:, start:end]\n",
    "        Y = self.data[:, start + 1: end + 1]\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    # We assume logits are (B, seq_len, out_dim)\n",
    "    max_logits = np.max(logits, axis=2, keepdims=True) # (B, seq_len, 1)\n",
    "    shifted_logits = logits - max_logits # (B, seq_len, out_dim)\n",
    "    \n",
    "    exp_logits = np.exp(shifted_logits) # (B, seq_len, out_dim)\n",
    "    exp_sum = np.sum(exp_logits, axis=2, keepdims=True) # (B, seq_len, 1)\n",
    "    \n",
    "    probs = exp_logits / exp_sum # (B, seq_len, out_dim)\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def sample(model: RNN, embedding: Embedding, dataset: TinyShakespeareDataset, start_char: str, length: int, temperature=0.8) -> str:\n",
    "    current_input = embedding.forward(np.array(dataset.stoi[start_char]).reshape(1, -1)) # Becomes (1, 1, embed_dim)\n",
    "    h = None\n",
    "    \n",
    "    output = \"\"\n",
    "    \n",
    "    for _ in range(length):\n",
    "        logits, h = model.forward(current_input, h)\n",
    "        # logits is (B, seq_len, out_dim) = (1, 1, vocab_size)\n",
    "        logits = logits / temperature\n",
    "        probs = softmax(logits).reshape(dataset.vocab_size) # (vocab_size,)\n",
    "        \n",
    "        next_ix = np.random.choice(dataset.vocab_size, p=probs)\n",
    "        output += dataset.itos[next_ix]\n",
    "        \n",
    "        current_input = embedding.forward(np.array(next_ix).reshape(1, -1)) # Becomes (1, 1, embed_dim)\n",
    "        \n",
    "    return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e85a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb07644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1e651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
